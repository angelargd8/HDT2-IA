{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "integrantes: \n",
    "\n",
    "            - Gerardo Pineda #22808\n",
    "\n",
    "            - Francis Aguilar #22243\n",
    "\n",
    "            - Angela Garcia #22869\n",
    "\n",
    "Enlace al repositio: \n",
    "https://github.com/angelargd8/HDT2-IA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Defina el proceso de decisi칩n de Markov (MDP) y explique sus componentes.\n",
    "    El proceso de decisi칩n de Markov (MDP), ayuda a modelar la toma de descisiones en situaciones donde los resultados son aleatorios y tiene bajo el control de decisor.Usualmente se usa en el area de IA, aprendizaje por refuerzo y la teor칤a de control.\n",
    "\n",
    "    Componentes: \n",
    "    - Estados: \n",
    "        Representan las posibles situaciones en las que el agente se puede encontrar.\n",
    "\n",
    "    - Acciones:\n",
    "        las decisiones/ movimientos que un agente puede tomar\n",
    "    \n",
    "    - Probabilidades de transicion: \n",
    "        Describe la probabilidad de moverse de un estado a otra, dado una accion.\n",
    "\n",
    "    - Recompensas:\n",
    "        Indican la recompensa inmediata luego de tomar una accion en un estado\n",
    "\n",
    "    - Horizonte de tiempo: \n",
    "        Si es finito o infinito, define el periodo durante en el que se toman desiciones\n",
    "\n",
    "    - Descuento:\n",
    "        factor de descuento entre 0 y 1 que reduce el valor de las recompensas futuras, lo que refleja es la preferencia por obtener las recomensas m치s temprano.\n",
    "\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre pol칤tica, evaluaci칩n de pol칤ticas, mejora de pol칤ticas e iteraci칩n de pol칤ticas\n",
    "en el contexto de los PDM.\n",
    "    - pol칤tica\n",
    "        define la forma en la que el agente toma descisiones.\n",
    "\n",
    "    - evaluaci칩n de pol칤ticas\n",
    "        calcula el valor esperado de seguir una politica en todos los estados posibles del PDM\n",
    "\n",
    "    - mejora de pol칤ticas\n",
    "        toma la politica actual y la modifica para mejorar su valor esperado de las recompensas acumuladas y de esta forma aumente\n",
    "\n",
    "    - iteraci칩n de pol칤ticas\n",
    "        combina la evaluaci칩n de politicas y la mejora de politicas para mejorar la politica en un ciclo iterativo.\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. 쮺칩mo influye en la toma de decisiones?\n",
    "\n",
    "    El factor de descuento es un parametro importante en los MDP, porque este se usa para determinar la importancia de las recompensas futuras en comparaci칩n ocn las recompensas inmediatas.\n",
    "\n",
    "    Influye en la toma de desiciones, cuando el 洧 es cercano a 1, significa que las recompensa futura casi tienen la misma importancia que las recompensas inmediatas.\n",
    "    Mientras que si el 洧 es cercano a 0, significa que las recompensas futuras tienen menos importancia que las recompensas inmediatas.\n",
    "\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteraci칩n de valores y de iteraci칩n de pol칤ticas para resolver MDP.\n",
    "\n",
    "    En la iteraci칩n de valores, las diferencias se encuentran con que se calcula los valores del estado directamente, que se actualizan los valores de los estados iterativamnte y se determinan la politica optima. \n",
    "    Luego por la parte de la iteracion de las politicas, en este se alterna entre evaluar y mejorar la politica. Este evalua la politica actual y la mejora iterativamente. \n",
    "\n",
    "\n",
    "5. 쮺u치les son algunos desaf칤os o limitaciones comunes asociados con la resoluci칩n de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desaf칤os.\n",
    "\n",
    "    Los desaf칤os con los que se puede encontrar con la resoluci칩n de MDP es que el numero de estados puede crecer exponencialmente, entonces se puede requerir un gran espacio de memoria y tiempo de computo. Similar a la dimensionalidad de los estados se debe de tomar en cuenta el espacio de las posibles acciones.\n",
    "    Para abordar esto, lo que se puede encontrar son representaciones compactas y eficientes de los estados.\n",
    "\n",
    "    Ya que tiene algoritmos de iteracion y politicas, pueden llegar a ser muy costosos computacionalmente, siendo bastante tardado en cuanto tiempo y necesite varios recursos. Una solucion a esto es asegurar la convergencia de los algoritmos, y asegurar que los algoritmos sean eficientes en cuanto a tiempo y recursos.\n",
    "\n",
    "\n",
    "    Otra dificultad que se puede encontrar es en el calculo de las probabilidades de transicion y recompensas, porque puede ser dificil y costoso, dependiendo de la cantidad de posibles transiciones y resultados.  Por otra parte, los MDP pueden ser no estocasticos, lo que puede hacer que la resolucion sea mas dificil. Adem치s, en el factor de descuento si se tiene un factor de descuento incorrecto puede afectar la resolucion del problema. \n",
    "\n",
    "    Para abordar estos desafios, lo que se puede tomar en cuenta es la reducci칩n de dimensionalidad, usar algorimos basados en modelo para el aprendizaje por refuerzo, tener tecnicas de muestreo y simulaci칩n para la estumacion de probabilidades y recompensas . \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
