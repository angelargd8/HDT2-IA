{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "integrantes: \n",
    "\n",
    "            - Gerardo Pineda #22808\n",
    "\n",
    "            - Francis Aguilar #22243\n",
    "\n",
    "            - Angela Garcia #22869\n",
    "\n",
    "Enlace al repositio: \n",
    "https://github.com/angelargd8/HDT2-IA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "    El proceso de decisión de Markov (MDP), ayuda a modelar la toma de descisiones en situaciones donde los resultados son aleatorios y tiene bajo el control de decisor.Usualmente se usa en el area de IA, aprendizaje por refuerzo y la teoría de control.\n",
    "\n",
    "    Componentes: \n",
    "    - Estados: \n",
    "        Representan las posibles situaciones en las que el agente se puede encontrar.\n",
    "\n",
    "    - Acciones:\n",
    "        las decisiones/ movimientos que un agente puede tomar\n",
    "    \n",
    "    - Probabilidades de transicion: \n",
    "        Describe la probabilidad de moverse de un estado a otra, dado una accion.\n",
    "\n",
    "    - Recompensas:\n",
    "        Indican la recompensa inmediata luego de tomar una accion en un estado\n",
    "\n",
    "    - Horizonte de tiempo: \n",
    "        Si es finito o infinito, define el periodo durante en el que se toman desiciones\n",
    "\n",
    "    - Descuento:\n",
    "        factor de descuento entre 0 y 1 que reduce el valor de las recompensas futuras, lo que refleja es la preferencia por obtener las recomensas más temprano.\n",
    "\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas\n",
    "en el contexto de los PDM.\n",
    "    - política\n",
    "        define la forma en la que el agente toma descisiones.\n",
    "\n",
    "    - evaluación de políticas\n",
    "        calcula el valor esperado de seguir una politica en todos los estados posibles del PDM\n",
    "\n",
    "    - mejora de políticas\n",
    "        toma la politica actual y la modifica para mejorar su valor esperado de las recompensas acumuladas y de esta forma aumente\n",
    "\n",
    "    - iteración de políticas\n",
    "        combina la evaluación de politicas y la mejora de politicas para mejorar la politica en un ciclo iterativo.\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "\n",
    "    El factor de descuento es un parametro importante en los MDP, porque este se usa para determinar la importancia de las recompensas futuras en comparación ocn las recompensas inmediatas.\n",
    "\n",
    "    Influye en la toma de desiciones, cuando el 𝛾 es cercano a 1, significa que las recompensa futura casi tienen la misma importancia que las recompensas inmediatas.\n",
    "    Mientras que si el 𝛾 es cercano a 0, significa que las recompensas futuras tienen menos importancia que las recompensas inmediatas.\n",
    "\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
    "\n",
    "    En la iteración de valores, las diferencias se encuentran con que se calcula los valores del estado directamente, que se actualizan los valores de los estados iterativamnte y se determinan la politica optima. \n",
    "    Luego por la parte de la iteracion de las politicas, en este se alterna entre evaluar y mejorar la politica. Este evalua la politica actual y la mejora iterativamente. \n",
    "\n",
    "\n",
    "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desafíos.\n",
    "\n",
    "    Los desafíos con los que se puede encontrar con la resolución de MDP es que el numero de estados puede crecer exponencialmente, entonces se puede requerir un gran espacio de memoria y tiempo de computo. Similar a la dimensionalidad de los estados se debe de tomar en cuenta el espacio de las posibles acciones.\n",
    "    Para abordar esto, lo que se puede encontrar son representaciones compactas y eficientes de los estados.\n",
    "\n",
    "    Ya que tiene algoritmos de iteracion y politicas, pueden llegar a ser muy costosos computacionalmente, siendo bastante tardado en cuanto tiempo y necesite varios recursos. Una solucion a esto es asegurar la convergencia de los algoritmos, y asegurar que los algoritmos sean eficientes en cuanto a tiempo y recursos.\n",
    "\n",
    "\n",
    "    Otra dificultad que se puede encontrar es en el calculo de las probabilidades de transicion y recompensas, porque puede ser dificil y costoso, dependiendo de la cantidad de posibles transiciones y resultados.  Por otra parte, los MDP pueden ser no estocasticos, lo que puede hacer que la resolucion sea mas dificil. Además, en el factor de descuento si se tiene un factor de descuento incorrecto puede afectar la resolucion del problema. \n",
    "\n",
    "    Para abordar estos desafios, lo que se puede tomar en cuenta es la reducción de dimensionalidad, usar algorimos basados en modelo para el aprendizaje por refuerzo, tener tecnicas de muestreo y simulación para la estumacion de probabilidades y recompensas . \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
