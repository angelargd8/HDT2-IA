{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "integrantes: \n",
    "\n",
    "            - Gerardo Pineda #22808\n",
    "\n",
    "            - Francis Aguilar #22243\n",
    "\n",
    "            - Angela Garcia #22869\n",
    "\n",
    "Enlace al repositio: \n",
    "https://github.com/angelargd8/HDT2-IA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Defina el proceso de decisi贸n de Markov (MDP) y explique sus componentes.\n",
    "    El proceso de decisi贸n de Markov (MDP), ayuda a modelar la toma de descisiones en situaciones donde los resultados son aleatorios y tiene bajo el control de decisor.Usualmente se usa en el area de IA, aprendizaje por refuerzo y la teor铆a de control.\n",
    "\n",
    "    Componentes: \n",
    "    - Estados: \n",
    "        Representan las posibles situaciones en las que el agente se puede encontrar.\n",
    "\n",
    "    - Acciones:\n",
    "        las decisiones/ movimientos que un agente puede tomar\n",
    "    \n",
    "    - Probabilidades de transicion: \n",
    "        Describe la probabilidad de moverse de un estado a otra, dado una accion.\n",
    "\n",
    "    - Recompensas:\n",
    "        Indican la recompensa inmediata luego de tomar una accion en un estado\n",
    "\n",
    "    - Horizonte de tiempo: \n",
    "        Si es finito o infinito, define el periodo durante en el que se toman desiciones\n",
    "\n",
    "    - Descuento:\n",
    "        factor de descuento entre 0 y 1 que reduce el valor de las recompensas futuras, lo que refleja es la preferencia por obtener las recomensas m谩s temprano.\n",
    "\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre pol铆tica, evaluaci贸n de pol铆ticas, mejora de pol铆ticas e iteraci贸n de pol铆ticas\n",
    "en el contexto de los PDM.\n",
    "    - pol铆tica\n",
    "        define la forma en la que el agente toma descisiones.\n",
    "\n",
    "    - evaluaci贸n de pol铆ticas\n",
    "        calcula el valor esperado de seguir una politica en todos los estados posibles del PDM\n",
    "\n",
    "    - mejora de pol铆ticas\n",
    "        toma la politica actual y la modifica para mejorar su valor esperado de las recompensas acumuladas y de esta forma aumente\n",
    "\n",
    "    - iteraci贸n de pol铆ticas\n",
    "        combina la evaluaci贸n de politicas y la mejora de politicas para mejorar la politica en un ciclo iterativo.\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. 驴C贸mo influye en la toma de decisiones?\n",
    "\n",
    "    El factor de descuento es un parametro importante en los MDP, porque este se usa para determinar la importancia de las recompensas futuras en comparaci贸n ocn las recompensas inmediatas.\n",
    "\n",
    "    Influye en la toma de desiciones, cuando el  es cercano a 1, significa que las recompensa futura casi tienen la misma importancia que las recompensas inmediatas.\n",
    "    Mientras que si el  es cercano a 0, significa que las recompensas futuras tienen menos importancia que las recompensas inmediatas.\n",
    "\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteraci贸n de valores y de iteraci贸n de pol铆ticas para resolver MDP.\n",
    "\n",
    "    En la iteraci贸n de valores, las diferencias se encuentran con que se calcula los valores del estado directamente, que se actualizan los valores de los estados iterativamnte y se determinan la politica optima. \n",
    "    Luego por la parte de la iteracion de las politicas, en este se alterna entre evaluar y mejorar la politica. Este evalua la politica actual y la mejora iterativamente. \n",
    "\n",
    "\n",
    "5. 驴Cu谩les son algunos desaf铆os o limitaciones comunes asociados con la resoluci贸n de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desaf铆os.\n",
    "\n",
    "    Los desaf铆os con los que se puede encontrar con la resoluci贸n de MDP es que el numero de estados puede crecer exponencialmente, entonces se puede requerir un gran espacio de memoria y tiempo de computo. Similar a la dimensionalidad de los estados se debe de tomar en cuenta el espacio de las posibles acciones.\n",
    "    Para abordar esto, lo que se puede encontrar son representaciones compactas y eficientes de los estados.\n",
    "\n",
    "    Ya que tiene algoritmos de iteracion y politicas, pueden llegar a ser muy costosos computacionalmente, siendo bastante tardado en cuanto tiempo y necesite varios recursos. Una solucion a esto es asegurar la convergencia de los algoritmos, y asegurar que los algoritmos sean eficientes en cuanto a tiempo y recursos.\n",
    "\n",
    "\n",
    "    Otra dificultad que se puede encontrar es en el calculo de las probabilidades de transicion y recompensas, porque puede ser dificil y costoso, dependiendo de la cantidad de posibles transiciones y resultados.  Por otra parte, los MDP pueden ser no estocasticos, lo que puede hacer que la resolucion sea mas dificil. Adem谩s, en el factor de descuento si se tiene un factor de descuento incorrecto puede afectar la resolucion del problema. \n",
    "\n",
    "    Para abordar estos desafios, lo que se puede tomar en cuenta es la reducci贸n de dimensionalidad, usar algorimos basados en modelo para el aprendizaje por refuerzo, tener tecnicas de muestreo y simulaci贸n para la estumacion de probabilidades y recompensas . \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Responda a cada de las siguientes preguntas de forma clara y lo m谩s completamente posible.  \n",
    "\n",
    "****1. Analice cr铆ticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisi贸n de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser v谩lidos y sus implicaciones para la toma de decisiones.****\n",
    "\n",
    "\n",
    "*Supuestos clave*\n",
    "- Memoria limitada: La decisi贸n de Markov indica que solo se necesita del estado actual para poder predecir la siguiente, que la transici贸n no necesita la informaci贸n previa al estado actual para poder transicionar. Esto es un problema si llegaramos a tener un caso en donde el historial es importante e informaci贸n relevante no est谩 presente en todos los estados. \n",
    "- Representaci贸n completa: El estado actual representa toda la informaci贸n necesaria para la toma de decisiones de predicciones. Aqu铆 es un problema por ejemplo si es un caso en donde la situaci贸n cambia con el tiempo, como podr谩 un solo estado almacenar toda la informaci贸n necesaria para predecir si tiene un entorno cambiante.\n",
    "- Estacionario: Las din谩micas de transici贸n no cambian con el tiempo. Si hay eventos externos que afectan el proceso, la predicci贸n se ve afectada porque no se adapta a los cambios. \n",
    "\n",
    "\n",
    "\n",
    "****2. Explore los desaf铆os de modelar la incertidumbre en los procesos de decisi贸n de Markov (MDP) y analice estrategias para una toma de decisiones s贸lida en entornos inciertos.****\n",
    "\n",
    "En general, la falta de datos hist贸ricos y la incertidumbre de cada caso, son los desafios m谩s grandes a los que se enfrenta el MDP.   \n",
    "- Incertidumbre en las transiciones\n",
    "- Incertidumbre en las recompensas \n",
    "- Cambios en el entorno\n",
    "- Modelo incompleto por falta de informaci贸n\n",
    "- Predicciones incorrectas\n",
    "\n",
    "Algunas de las estrategias para una toma de decisiones m谩s s贸lida son: \n",
    "- Balancear la exploraci贸n de los datos para aumentar la precisi贸n del modelo. \n",
    "- Minimizar la varianza para optimizar los resultados\n",
    "- Apoyo en modelos bayesianos\n",
    "- Actualizaci贸n constante del modelo seg煤n variantes\n",
    "- Utilizar pol铆ticas m谩s robustas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states  16\n",
      "Number of actions  4\n",
      "Transitions for state 0 and action LEFT are\n",
      "  [(1.0, 0, 0.0, False)]\n",
      "Transitions for state 0 and action UP are\n",
      "  [(1.0, 0, 0.0, False)]\n",
      "Transitions for state 11 and action LEFT are\n",
      "  [(1.0, 11, 0, True)]\n",
      "Transitions for state 11 and action UP are\n",
      "  [(1.0, 11, 0, True)]\n",
      "Transitions for state 15 and action LEFT are\n",
      "  [(1.0, 15, 0, True)]\n",
      "Transitions for state 15 and action UP are\n",
      "  [(1.0, 15, 0, True)]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from MarkovDecisionProcess import MarkovDecisionProcess as MDP\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.reset()\n",
    "env.render()\n",
    "mdp = MDP(env.observation_space.n, env.action_space.n, env.unwrapped.P)\n",
    "\n",
    "print(\"Number of states \", mdp.num_states)\n",
    "print(\"Number of actions \", mdp.num_actions)\n",
    "\n",
    "sample_actions = {'LEFT':0, 'UP':3}\n",
    "sample_states = [0,11,15]\n",
    "for s in sample_states:\n",
    "    for a in sample_actions.keys():\n",
    "        print(f\"Transitions for state {s} and action {a} are\\n \", mdp.P[s][sample_actions[a]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
