{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "integrantes: \n",
    "\n",
    "            - Gerardo Pineda #22808\n",
    "\n",
    "            - Francis Aguilar #22243\n",
    "\n",
    "            - Angela Garcia #22869\n",
    "\n",
    "Enlace al repositio: \n",
    "https://github.com/angelargd8/HDT2-IA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Defina el proceso de decisi√≥n de Markov (MDP) y explique sus componentes.\n",
    "    El proceso de decisi√≥n de Markov (MDP), ayuda a modelar la toma de descisiones en situaciones donde los resultados son aleatorios y tiene bajo el control de decisor.Usualmente se usa en el area de IA, aprendizaje por refuerzo y la teor√≠a de control.\n",
    "\n",
    "    Componentes: \n",
    "    - Estados: \n",
    "        Representan las posibles situaciones en las que el agente se puede encontrar.\n",
    "\n",
    "    - Acciones:\n",
    "        las decisiones/ movimientos que un agente puede tomar\n",
    "    \n",
    "    - Probabilidades de transicion: \n",
    "        Describe la probabilidad de moverse de un estado a otra, dado una accion.\n",
    "\n",
    "    - Recompensas:\n",
    "        Indican la recompensa inmediata luego de tomar una accion en un estado\n",
    "\n",
    "    - Horizonte de tiempo: \n",
    "        Si es finito o infinito, define el periodo durante en el que se toman desiciones\n",
    "\n",
    "    - Descuento:\n",
    "        factor de descuento entre 0 y 1 que reduce el valor de las recompensas futuras, lo que refleja es la preferencia por obtener las recomensas m√°s temprano.\n",
    "\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre pol√≠tica, evaluaci√≥n de pol√≠ticas, mejora de pol√≠ticas e iteraci√≥n de pol√≠ticas\n",
    "en el contexto de los PDM.\n",
    "    - pol√≠tica\n",
    "        define la forma en la que el agente toma descisiones.\n",
    "\n",
    "    - evaluaci√≥n de pol√≠ticas\n",
    "        calcula el valor esperado de seguir una politica en todos los estados posibles del PDM\n",
    "\n",
    "    - mejora de pol√≠ticas\n",
    "        toma la politica actual y la modifica para mejorar su valor esperado de las recompensas acumuladas y de esta forma aumente\n",
    "\n",
    "    - iteraci√≥n de pol√≠ticas\n",
    "        combina la evaluaci√≥n de politicas y la mejora de politicas para mejorar la politica en un ciclo iterativo.\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¬øC√≥mo influye en la toma de decisiones?\n",
    "\n",
    "    El factor de descuento es un parametro importante en los MDP, porque este se usa para determinar la importancia de las recompensas futuras en comparaci√≥n ocn las recompensas inmediatas.\n",
    "\n",
    "    Influye en la toma de desiciones, cuando el ùõæ es cercano a 1, significa que las recompensa futura casi tienen la misma importancia que las recompensas inmediatas.\n",
    "    Mientras que si el ùõæ es cercano a 0, significa que las recompensas futuras tienen menos importancia que las recompensas inmediatas.\n",
    "\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteraci√≥n de valores y de iteraci√≥n de pol√≠ticas para resolver MDP.\n",
    "\n",
    "    En la iteraci√≥n de valores, las diferencias se encuentran con que se calcula los valores del estado directamente, que se actualizan los valores de los estados iterativamnte y se determinan la politica optima. \n",
    "    Luego por la parte de la iteracion de las politicas, en este se alterna entre evaluar y mejorar la politica. Este evalua la politica actual y la mejora iterativamente. \n",
    "\n",
    "\n",
    "5. ¬øCu√°les son algunos desaf√≠os o limitaciones comunes asociados con la resoluci√≥n de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desaf√≠os.\n",
    "\n",
    "    Los desaf√≠os con los que se puede encontrar con la resoluci√≥n de MDP es que el numero de estados puede crecer exponencialmente, entonces se puede requerir un gran espacio de memoria y tiempo de computo. Similar a la dimensionalidad de los estados se debe de tomar en cuenta el espacio de las posibles acciones.\n",
    "    Para abordar esto, lo que se puede encontrar son representaciones compactas y eficientes de los estados.\n",
    "\n",
    "    Ya que tiene algoritmos de iteracion y politicas, pueden llegar a ser muy costosos computacionalmente, siendo bastante tardado en cuanto tiempo y necesite varios recursos. Una solucion a esto es asegurar la convergencia de los algoritmos, y asegurar que los algoritmos sean eficientes en cuanto a tiempo y recursos.\n",
    "\n",
    "\n",
    "    Otra dificultad que se puede encontrar es en el calculo de las probabilidades de transicion y recompensas, porque puede ser dificil y costoso, dependiendo de la cantidad de posibles transiciones y resultados.  Por otra parte, los MDP pueden ser no estocasticos, lo que puede hacer que la resolucion sea mas dificil. Adem√°s, en el factor de descuento si se tiene un factor de descuento incorrecto puede afectar la resolucion del problema. \n",
    "\n",
    "    Para abordar estos desafios, lo que se puede tomar en cuenta es la reducci√≥n de dimensionalidad, usar algorimos basados en modelo para el aprendizaje por refuerzo, tener tecnicas de muestreo y simulaci√≥n para la estumacion de probabilidades y recompensas . \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Responda a cada de las siguientes preguntas de forma clara y lo m√°s completamente posible.  \n",
    "\n",
    "****1. Analice cr√≠ticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisi√≥n de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser v√°lidos y sus implicaciones para la toma de decisiones.****\n",
    "\n",
    "\n",
    "*Supuestos clave*\n",
    "- Memoria limitada: La decisi√≥n de Markov indica que solo se necesita del estado actual para poder predecir la siguiente, que la transici√≥n no necesita la informaci√≥n previa al estado actual para poder transicionar. Esto es un problema si llegaramos a tener un caso en donde el historial es importante e informaci√≥n relevante no est√° presente en todos los estados. \n",
    "- Representaci√≥n completa: El estado actual representa toda la informaci√≥n necesaria para la toma de decisiones de predicciones. Aqu√≠ es un problema por ejemplo si es un caso en donde la situaci√≥n cambia con el tiempo, como podr√° un solo estado almacenar toda la informaci√≥n necesaria para predecir si tiene un entorno cambiante.\n",
    "- Estacionario: Las din√°micas de transici√≥n no cambian con el tiempo. Si hay eventos externos que afectan el proceso, la predicci√≥n se ve afectada porque no se adapta a los cambios. \n",
    "\n",
    "\n",
    "\n",
    "****2. Explore los desaf√≠os de modelar la incertidumbre en los procesos de decisi√≥n de Markov (MDP) y analice estrategias para una toma de decisiones s√≥lida en entornos inciertos.****\n",
    "\n",
    "En general, la falta de datos hist√≥ricos y la incertidumbre de cada caso, son los desafios m√°s grandes a los que se enfrenta el MDP.   \n",
    "- Incertidumbre en las transiciones\n",
    "- Incertidumbre en las recompensas \n",
    "- Cambios en el entorno\n",
    "- Modelo incompleto por falta de informaci√≥n\n",
    "- Predicciones incorrectas\n",
    "\n",
    "Algunas de las estrategias para una toma de decisiones m√°s s√≥lida son: \n",
    "- Balancear la exploraci√≥n de los datos para aumentar la precisi√≥n del modelo. \n",
    "- Minimizar la varianza para optimizar los resultados\n",
    "- Apoyo en modelos bayesianos\n",
    "- Actualizaci√≥n constante del modelo seg√∫n variantes\n",
    "- Utilizar pol√≠ticas m√°s robustas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inicio: (3, 0)\n",
      "meta: (0, 3)\n",
      "numero de hoyos: 1 \n",
      "\n",
      "FFFG\n",
      "FFFF\n",
      "HFFF\n",
      "SFFF\n",
      "\n",
      "Pol√≠tica √≥ptima:\n",
      "‚Üí ‚Üí ‚Üí ‚Üê\n",
      "‚Üí ‚Üí ‚Üí ‚Üë\n",
      "‚Üê ‚Üí ‚Üí ‚Üë\n",
      "‚Üí ‚Üí ‚Üí ‚Üë\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Configuraci√≥n reproducible\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def crear_frozenlake_custom(inicio, meta, num_hoyos):\n",
    "    size = 4\n",
    "    mapa = [['F' for _ in range(size)] for _ in range(size)]\n",
    "    \n",
    "    # Colocar inicio y meta\n",
    "    mapa[inicio[0]][inicio[1]] = 'S'\n",
    "    mapa[meta[0]][meta[1]] = 'G'\n",
    "    \n",
    "    # Generar hoyos aleatorios\n",
    "    posiciones_disponibles = [(i, j) for i in range(size) for j in range(size)\n",
    "                              if (i, j) != inicio and (i, j) != meta]\n",
    "    \n",
    "    hoyos = random.sample(posiciones_disponibles, num_hoyos)\n",
    "    # print('hoyos:')\n",
    "    # print(hoyos)\n",
    "\n",
    "    for (i, j) in hoyos:\n",
    "        mapa[i][j] = 'H'\n",
    "    \n",
    "    desc = [\"\".join(row) for row in mapa]\n",
    "    return desc\n",
    "\n",
    "\n",
    "def mostrar_mapa(desc):\n",
    "    for row in desc:\n",
    "        print(row)\n",
    "\n",
    "\n",
    "# Definir posiciones y cantidad de hoyos\n",
    "\n",
    "inicios_posibles = [(0, 0), (0, 3), (3,3), (3, 0)]\n",
    "metas_posibles = [(3, 3), (3,0), (0,0),(0,3)]\n",
    "\n",
    "random_posicion = random.randint(0, 3)\n",
    "inicio = inicios_posibles[random_posicion]\n",
    "meta = metas_posibles[random_posicion]\n",
    "\n",
    "\n",
    "num_hoyos = random.randint(1, 3)\n",
    "print('inicio:', inicio)\n",
    "print('meta:', meta)\n",
    "print('numero de hoyos:', num_hoyos, '\\n')\n",
    "\n",
    "# Crear mapa personalizado\n",
    "desc = crear_frozenlake_custom(inicio, meta, num_hoyos)\n",
    "mostrar_mapa(desc)\n",
    "\n",
    "# Crear entorno personalizado\n",
    "env = gym.make(\"FrozenLake-v1\", desc=desc, is_slippery=False)\n",
    "state = env.reset()  # Esto devuelve el estado inicial\n",
    "env.render()\n",
    "\n",
    "# Par√°metros del agente MDP\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "gamma = 0.9\n",
    "theta = 0.0001\n",
    "\n",
    "# Inicializar funci√≥n de valor\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "# Iteraci√≥n de valores\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in range(num_states):\n",
    "        v = V[s]\n",
    "        q_sa = []\n",
    "        for a in range(num_actions):\n",
    "            q = 0\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q += prob * (reward + gamma * V[next_state])\n",
    "            q_sa.append(q)\n",
    "        V[s] = max(q_sa)\n",
    "        delta = max(delta, abs(v - V[s]))\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Extraer pol√≠tica √≥ptima\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "for s in range(num_states):\n",
    "    q_sa = []\n",
    "    for a in range(num_actions):\n",
    "        q = 0\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q += prob * (reward + gamma * V[next_state])\n",
    "        q_sa.append(q)\n",
    "    policy[s] = np.argmax(q_sa)\n",
    "\n",
    "# Mostrar pol√≠tica en forma de flechas\n",
    "acciones = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
    "print(\"\\nPol√≠tica √≥ptima:\")\n",
    "for i in range(4):\n",
    "    fila = policy[i * 4:(i + 1) * 4]\n",
    "    print(' '.join(acciones[a] for a in fila))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
